apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: '${CLUSTER_NAME}'
  labels:
    cni: "${CLUSTER_NAME}-crs-0"
spec:
  clusterNetwork:
    services:
      cidrBlocks: ['${DOCKER_SERVICE_CIDRS}']
    pods:
      cidrBlocks: ['${DOCKER_POD_CIDRS}']
    serviceDomain: '${DOCKER_SERVICE_DOMAIN}'
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: DockerCluster
    name: '${CLUSTER_NAME}'
  controlPlaneRef:
    kind: KubeadmControlPlane
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    name: "${CLUSTER_NAME}-control-plane"
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: DockerCluster
metadata:
  name: '${CLUSTER_NAME}'
---
kind: KubeadmControlPlane
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
metadata:
  name: "${CLUSTER_NAME}-control-plane"
  labels:
    kcp-adoption.step2: ""
spec:
  replicas: ${CONTROL_PLANE_MACHINE_COUNT}
  machineTemplate:
    infrastructureRef:
      kind: DockerMachineTemplate
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      name: "${CLUSTER_NAME}-control-plane"
  kubeadmConfigSpec:
    clusterConfiguration:
      controllerManager:
        extraArgs:
          enable-hostpath-provisioner: 'true'
      apiServer:
        certSANs:
        - localhost
        - 127.0.0.1
        - 0.0.0.0
        # host.docker.internal is required by kubetest when running on MacOS because of the way ports are proxied.
        - host.docker.internal
    initConfiguration:
      nodeRegistration:
        kubeletExtraArgs:
          eviction-hard: 'nodefs.available<0%,nodefs.inodesFree<0%,imagefs.available<0%'
    joinConfiguration:
      nodeRegistration:
        kubeletExtraArgs:
          eviction-hard: 'nodefs.available<0%,nodefs.inodesFree<0%,imagefs.available<0%'
  version: "${KUBERNETES_VERSION}"
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: DockerMachineTemplate
metadata:
  name: "${CLUSTER_NAME}-control-plane"
spec:
  template:
    spec:
      extraMounts:
        - containerPath: "/var/run/docker.sock"
          hostPath: "/var/run/docker.sock"
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  annotations:
    cluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size: "5"
    cluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size: "1"
  name: "${CLUSTER_NAME}-md-0"
spec:
  clusterName: "${CLUSTER_NAME}"
  replicas: ${WORKER_MACHINE_COUNT}
  template:
    spec:
      clusterName: "${CLUSTER_NAME}"
      version: "${KUBERNETES_VERSION}"
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
          name: "${CLUSTER_NAME}-md-0"
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1alpha4
        kind: KubemarkMachineTemplate
        name: "${CLUSTER_NAME}-md-0"
---
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: KubeadmConfigTemplate
metadata:
  name: "${CLUSTER_NAME}-md-0"
  namespace: default
spec:
  template:
    spec:
      joinConfiguration:
        nodeRegistration:
          name: ''
---
apiVersion: infrastructure.cluster.x-k8s.io/v1alpha4
kind: KubemarkMachineTemplate
metadata:
  name: "${CLUSTER_NAME}-md-0"
spec:
  template:
    spec:
      extraMounts:
        - name: containerd-sock
          containerPath: /run/containerd/containerd.sock
          hostPath: /run/containerd/containerd.sock
          type: Socket
---
# ConfigMap object referenced by the ClusterResourceSet object and with
# the CNI resource defined in the test config file
apiVersion: v1
kind: ConfigMap
metadata:
  name: "cni-${CLUSTER_NAME}-crs-0"
data: ${CNI_RESOURCES}
binaryData:
---
# ClusterResourceSet object with
# a selector that targets all the Cluster with label cni=${CLUSTER_NAME}-crs-0
apiVersion: addons.cluster.x-k8s.io/v1beta1
kind: ClusterResourceSet
metadata:
  name:  "${CLUSTER_NAME}-crs-0"
spec:
  strategy: ApplyOnce
  clusterSelector:
    matchLabels:
      cni: "${CLUSTER_NAME}-crs-0"
  resources:
    - name: "cni-${CLUSTER_NAME}-crs-0"
      kind: ConfigMap
